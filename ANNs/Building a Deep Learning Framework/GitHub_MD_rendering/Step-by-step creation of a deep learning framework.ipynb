{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Step-by-step creation of a deep learning framework\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan of attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Our main goal** is to design an OO ANNs, that to say an Object-Oriented Artifical Neural Network.\n",
    "- We are going to create a `Tensor` class and will modify it 6 times.\n",
    "- Every time we add something weill add an integer so we'll keep track of what we are changing.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #1 - Create a tensor support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- A **vector** is a one-dimensional tensor. \n",
    "- A **matrix** is a two-dimensional tensor.\n",
    "- A higher dimensions are referred to as **n-dimensional tensors.**\n",
    "- Since we want to create something general, we then create a framework for tensor only.\n",
    "- `__str__ `function is supposed to return a human-readable format, which is good for logging or to \n",
    "display some information about the object. \n",
    "- `__repr__` function is supposed to return an “official” string representation of the object, which \n",
    "can be used to construct the object again.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Tensor_V0(object):\n",
    "\n",
    "    \"\"\"Tensor class\n",
    "\n",
    "    Stores all the numerical information in a NumPy array (self.data)\n",
    "    Supports one tensor operation (addition)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"__init__ method\n",
    "\n",
    "        Given a list makes it into an array\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = np.array(data)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"__add__ method\n",
    "\n",
    "        “+” is just syntactic sugar around the “__add__” method\n",
    "        so x+y is equivalent to x.__add__(y)\n",
    "        \"\"\"\n",
    "\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"__repr__ method\n",
    "\n",
    "        Returns the object representation in string format.\n",
    "        Returns an “official” string representation of the object,\n",
    "        which can be used to construct th eobject again.\n",
    "        \"\"\"\n",
    "\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"__str__ method\n",
    "\n",
    "        Returns the string representation of the object. This method is called \n",
    "        when print() or str() function is invoked on an object. Returns a\n",
    "        human-readable format.\n",
    "        \"\"\"\n",
    "\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'array([1, 2, 3, 4, 5])'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Official string representation.\n",
    "repr(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now you can use it to recreate it!\n",
    "np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1 2 3 4 5]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# human-readable representation\n",
    "str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  4,  6,  8, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding two tensors\n",
    "x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Tensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check type\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'data']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See all class methods and instance attribute\n",
    "dir(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is x an instance of the class Tensor?\n",
    "isinstance(x, Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #2 - adding back propgation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Backpropagation (aka backpro) is just a fancy name to compute the gradient and propagat back. The propgation is needed because we have more than one layers.\n",
    "- To be precise backprop **does not** compute gradient what is does. It just propagates it from output to input.\n",
    "- Simplistically it involves the computation the gradient at the output of the network which is then propagate all the wait back while updatating the weights on each layers. This is done untill a convergence criterion is met\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor_V1(object):\n",
    "\n",
    "    \"\"\"Tensor class\n",
    "\n",
    "    Stores all the numerical information in a NumPy array (self.data)\n",
    "    Supports one tensor operation (addition)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, creators=None, creation_op=None, verboseCall=True):\n",
    "        \"\"\"__init__ method\n",
    "\n",
    "        data string\n",
    "\n",
    "        creators  string\n",
    "            list containing any tensors used in the creation of the\n",
    "            current tensor (which defaults to None). \n",
    "        creation_op string\n",
    "            string describes the operation between the two creator tensor.\n",
    "        verboseCall Bool\n",
    "            True if you want to print the method being called\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.verboseCall = verboseCall\n",
    "\n",
    "    def myPrintDecorator(func):\n",
    "        \"\"\"My print decorator\n",
    "\n",
    "        Print the name of the method when it is called.\n",
    "        \"\"\"\n",
    "\n",
    "        def printCallNameMethod(self, *args, **kwargs):\n",
    "            if self.verboseCall == True:\n",
    "                print(\"Using method\", func.__name__)\n",
    "            instance = func(self, *args, **kwargs)\n",
    "            return instance\n",
    "        return printCallNameMethod\n",
    "\n",
    "    @myPrintDecorator\n",
    "    def backward(self, grad):\n",
    "        self.grad = grad\n",
    "        if(self.creation_op == \"add\"):\n",
    "            self.creators[0].backward(grad)\n",
    "            self.creators[1].backward(grad)\n",
    "\n",
    "    @myPrintDecorator\n",
    "    def __add__(self, other):\n",
    "        \"\"\"__add__ method\n",
    "\n",
    "        “+” is just syntactic sugar around the “__add__” method\n",
    "        so x+y is equivalent to x.__add__(y).\n",
    "\n",
    "        ensors x and y are added together, z has two creators\n",
    "        \"\"\"\n",
    "        return Tensor_V1(self.data + other.data,\n",
    "                         creators=[self, other],\n",
    "                         creation_op=\"add\")\n",
    "\n",
    "    @myPrintDecorator\n",
    "    def __repr__(self):\n",
    "        \"\"\"__repr__ method\n",
    "        Returns the object representation in string format.\n",
    "        Returns an “official” string representation of the object,\n",
    "        which can be used to construct th eobject again.\n",
    "        \"\"\"\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    @myPrintDecorator\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns the string representation of the object. This method is called \n",
    "        when print() or str() function is invoked on an object. Returns a\n",
    "        human-readable format.\n",
    "        \"\"\"\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- To create the backward pass we'll use the concept of **graph**.\n",
    "- To do this we first assign two new attributes:\n",
    "    - `creators = None` = list containing any tensors used in the creation of the current tensor.\n",
    "    - `creation_op = None` = string describes the operation between the two creator tensor.\n",
    "\n",
    "\n",
    "- `z = x + y` creates a **computation graph** with three nodes (x, y, and z) and two edges (z -> x and z -> y). Each edge is labeled \n",
    "by the creation_op add. This graph allows you to recursively backpropagate gradients.\n",
    "- This notion graph gets built during forward propagation on-the-fly and is called **dynamic computation graph**. This is in constrast with another method called **stati computational graph**.\n",
    "\n",
""   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create two tensor\n",
    "x = Tensor_V1([1, 2, 3, 4, 5])\n",
    "y = Tensor_V1([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# You can now see why we use the attribute none at the begging.\n",
    "# These are the inputs thus they do not have neither a cretor none an operation attached to\n",
    "# unless we do explicitly as int the following case\n",
    "print(y.creation_op)\n",
    "print(y.creators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "[[1, 3], [2]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we are going to create another tensor which has a creators and \n",
    "creation_op attributes given at the instantiation time\n",
    "\"\"\"\n",
    "xDummy = Tensor_V1([1, 2, 3, 4, 5], creators=[[1, 3], [2]], creation_op=\"test\")\n",
    "print(xDummy.creation_op)\n",
    "print(xDummy.creators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method __add__\n"
     ]
    }
   ],
   "source": [
    "# Going ot back to our example, let's build sum our two tensor\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method __repr__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2,  4,  6,  8, 10])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method __str__\n",
      "[ 2  4  6  8 10]\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method __repr__\n",
      "Using method __repr__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3, 4, 5]), array([1, 2, 3, 4, 5])]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the creators\n",
    "z.creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the operation\n",
    "z.creation_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **How do we propagate the gradient?** and we have not asked **How do we compute the gradient?!**\n",
    "- Let's say we have a gradients which is passed to z. Apply backpropagation means the gradient is applied to z parents: x and y. \n",
    "- Further, backpropagating through addition means also applying addition when backpropagating. \n",
    "- In this case, because there’s only one gradient to add into x or y, you copy the gradient from z onto x and y.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dummy gradient\n",
    "gradients = Tensor_V1(np.array([1,1,1,1,1]))\n",
    "# This will work as well\n",
    "#gradients = Tensor_V1([1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n"
     ]
    }
   ],
   "source": [
    "z.backward(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method __str__\n",
      "[1 1 1 1 1]\n",
      "Using method __str__\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# the gradient is passed wthouht change as we wanted\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Perhaps the **most elegant part** of this form of autograd is that it works recursively as well, because each \n",
    "vector calls .backward() on all of its self.creators:\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method __add__\n",
      "Using method __add__\n",
      "Using method __add__\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method __str__\n",
      "[1 1 1 1 1]\n",
      "Using method __str__\n",
      "[1 1 1 1 1]\n",
      "Using method __str__\n",
      "[1 1 1 1 1]\n",
      "Using method __str__\n",
      "[1 1 1 1 1]\n",
      "Using method __str__\n",
      "[1 1 1 1 1]\n",
      "Using method __str__\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor_V1([1, 2, 3, 4, 5])\n",
    "b = Tensor_V1([2, 2, 2, 2, 2])\n",
    "c = Tensor_V1([5, 4, 3, 2, 1])\n",
    "d = Tensor_V1([-1, -2, -3, -4, -5])\n",
    "e = a+b\n",
    "f = c+d\n",
    "g = e+f\n",
    "gradients = Tensor_V1([1, 1, 1, 1, 1])\n",
    "# We apply the gradient to the last element and we'll show how this is propgated backword\n",
    "g.backward(gradients)\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)\n",
    "print(d.grad)\n",
    "print(e.grad)\n",
    "print(f.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #3 - Tensors that are used multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The current version of Tensor supports backpropagating into a variable only once. \n",
    "- But sometimes, during forward propagation, you’ll use the same tensor multiple times (the weights of a neural network), and thus multiple parts of the graph will backpropagate gradients into the same tensor.\n",
    "- But the code will currently compute the INCORRECT gradient when backpropagating into a variable that was used multiple times.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method __add__\n",
      "Using method __add__\n",
      "Using method __add__\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "Using method backward\n",
      "[False False False False False]\n",
      "Using method __str__\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor_V1([1, 2, 3, 4, 5])\n",
    "b = Tensor_V1([2, 2, 2, 2, 2])\n",
    "c = Tensor_V1([5, 4, 3, 2, 1])\n",
    "\n",
    "d = a+b\n",
    "e = b+c\n",
    "f = d+e\n",
    "gradients = Tensor_V1(np.array([1, 1, 1, 1, 1]))\n",
    "f.backward(gradients)\n",
    "# We know that the gradient of b should be [2,2,2,2,2] and not [1,1,1,1,1]\n",
    "print(b.grad.data == np.array([2, 2, 2, 2, 2]))\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- In this example, the b variable is used twice in the process of creating f. \n",
    "- Thus, its gradient should be the sum of two derivatives: [2,2,2,2,2]. \n",
    "- Shown here is the resulting graph created by this chain of operations. \n",
    "- Notice there are now two pointers pointing into b: so, it should be the sum of the gradient coming from both e and d. \n",
    "- But the current implementation of Tensor merely overwrites each derivative with the previous. First, d applies  its gradient, and then it gets overwritten with the gradient from e. \n",
    "- We need to change the way gradients are written.\n",
    "\n",
""   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor_V3 (object):\n",
    "\n",
    "    \"\"\"Tensor class\n",
    "\n",
    "    Stores all the numerical information in a NumPy array (self.data), and it \n",
    "    supports one tensor operation (addition).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \"\"\"__init__ method\n",
    "\n",
    "        Given a list makes it into an array\n",
    "        Each tensor has now two new attributes:\n",
    "        creators   :: list containing any tensors used in the creation of the\n",
    "                      current tensor (which defaults to None). \n",
    "        creaion_op :: \n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if(id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "\n",
    "        # Keeping track of how many children a tensor has\n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Checks whether a tensor has received the \n",
    "        correct number of gradients from each child\n",
    "        \"\"\"\n",
    "\n",
    "        for id, cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # Checks to make sure you can backpropagate\n",
    "                # or wheter you are waiting for a gradient\n",
    "                # in which case decrement the counter\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                # Accumulates gradients from several children\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "\n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"__add__ method\n",
    "\n",
    "        “+” is just syntactic sugar around the “__add__” method\n",
    "        so x+y is equivalent to x.__add__(y).\n",
    "\n",
    "        ensors x and y are added together, z has two creators\n",
    "        \"\"\"\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor_V3(self.data + other.data,\n",
    "                             autograd=True,\n",
    "                             creators=[self, other],\n",
    "                             creation_op=\"add\")\n",
    "        return Tensor_V3(self.data + other.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"__repr__ method\n",
    "        Returns the object representation in string format.\n",
    "        Returns an “official” string representation of the object,\n",
    "        which can be used to construct th eobject again.\n",
    "        \"\"\"\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns the string representation of the object. This method is called \n",
    "        when print() or str() function is invoked on an object. Returns a\n",
    "        human-readable format.\n",
    "        \"\"\"\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor_V3([1, 2, 3, 4, 5], autograd=True)\n",
    "b = Tensor_V3([2, 2, 2, 2, 2], autograd=True)\n",
    "c = Tensor_V3([5, 4, 3, 2, 1], autograd=True)\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
    "print(b.grad.data == np.array([2, 2, 2, 2, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Additionally, you create a self.children counter that counts the number of gradients received from each  child during backpropagation. \n",
    "- This way, you also prevent a variable from accidentally backpropagating from the same child twice (which throws an exception).\n",
    "- The second added feature is a new function with the rather verbose name all_children_ grads_accounted_for(). \n",
    "- The purpose of this function is to compute whether a tensor has received gradients from all of its children in the graph. \n",
    "- Normally, whenever .backward() is called on an intermediate variable in a graph, it immediately calls .backward() on its parents. But because some variables receive their gradient value from multiple parents, each\n",
    "variable needs to wait to call .backward() on its parents until it has the final gradient locally.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #4 - Adding support for additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor_V4(object):\n",
    "\n",
    "    \"\"\"Tensor class\n",
    "\n",
    "    Stores all the numerical information in a NumPy array (self.data), and it \n",
    "    supports one tensor operation (addition).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \"\"\"__init__ method\n",
    "\n",
    "        Given a list makes it into an array\n",
    "        Each tensor has now two new attributes:\n",
    "        creators   :: list containing any tensors used in the creation of the\n",
    "                      current tensor (which defaults to None). \n",
    "        creaion_op :: \n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if(id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "\n",
    "        # Keeping track of how many children a tensor has\n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Checks whether a tensor has received the \n",
    "        correct number of gradients from each child\n",
    "        \"\"\"\n",
    "\n",
    "        for id, cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                # Checks to make sure you can backpropagate\n",
    "                # or wheter you are waiting for a gradient\n",
    "                # in which case decrement the counter\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                # Accumulates gradients from several children\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "\n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "\n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new = Tensor_V4(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor_V4(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    act = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"__add__ method\n",
    "\n",
    "        “+” is just syntactic sugar around the “__add__” method\n",
    "        so x+y is equivalent to x.__add__(y).\n",
    "\n",
    "        ensors x and y are added together, z has two creators\n",
    "        \"\"\"\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor_V4(self.data + other.data,\n",
    "                             autograd=True,\n",
    "                             creators=[self, other],\n",
    "                             creation_op=\"add\")\n",
    "        return Tensor_V4(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor_V4(self.data * -1,\n",
    "                             autograd=True,\n",
    "                             creators=[self],\n",
    "                             creation_op=\"neg\")\n",
    "        return Tensor_V4(self.data * -1)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor_V4(self.data - other.data,\n",
    "                             autograd=True,\n",
    "                             creators=[self, other],\n",
    "                             creation_op=\"sub\")\n",
    "        return Tensor_V4(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor_V4(self.data * other.data,\n",
    "                             autograd=True,\n",
    "                             creators=[self, other],\n",
    "                             creation_op=\"mul\")\n",
    "        return Tensor_V4(self.data * other.data)\n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor_V4(self.data.sum(dim),\n",
    "                             autograd=True,\n",
    "                             creators=[self],\n",
    "                             creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor_V4(self.data.sum(dim))\n",
    "\n",
    "    def expand(self, dim, copies):\n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        if(self.autograd):\n",
    "            return Tensor_V4(new_data,\n",
    "                             autograd=True,\n",
    "                             creators=[self],\n",
    "                             creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor_V4(new_data)\n",
    "\n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor_V4(self.data.transpose(),\n",
    "                             autograd=True,\n",
    "                             creators=[self],\n",
    "                             creation_op=\"transpose\")\n",
    "        return Tensor_V4(self.data.transpose())\n",
    "\n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor_V4(self.data.dot(x.data),\n",
    "                             autograd=True,\n",
    "                             creators=[self, x],\n",
    "                             creation_op=\"mm\")\n",
    "        return Tensor_V4(self.data.dot(x.data))\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"__repr__ method\n",
    "        Returns the object representation in string format.\n",
    "        Returns an “official” string representation of the object,\n",
    "        which can be used to construct th eobject again.\n",
    "        \"\"\"\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns the string representation of the object. This method is called \n",
    "        when print() or str() function is invoked on an object. Returns a\n",
    "        human-readable format.\n",
    "        \"\"\"\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor_V4([1,2,3,4,5], autograd=True) \n",
    "b = Tensor_V4([2,2,2,2,2], autograd=True) \n",
    "c = Tensor_V4([5,4,3,2,1], autograd=True)\n",
    "d = a + (-b) \n",
    "e = (-b) + c \n",
    "f=d+e\n",
    "f.backward(Tensor_V4(np.array([1,1,1,1,1]))) \n",
    "print(b.grad.data == np.array([-2,-2,-2,-2,-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #5 - Compared against a manually derived back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We'll now manually implement the back propagation and compared it against the one implemented in the class.\n",
    "- `data` has shape 4x2\n",
    "- `target` has shape 4x1\n",
    "- `weights_0_1` has shape 2x3. Please note that 2 is because we have two features and 3 could have been different.\n",
    "- `weights_1_2` has shape 3x1. Please note that that 1 is because the target has only one column and 3 is because the previous layer has 3 columns.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (4, 2)\n",
      "Target shape (4, 1)\n",
      "First weight layer shape (2, 3)\n",
      "Second weight layer shape (3, 1)\n",
      "\n",
      "Iteration No:  0\n",
      "Start forward pass\n",
      "In (4, 2) (2, 3)  Out (4, 3)\n",
      "In (4, 3) (3, 1)  Out (4, 1)\n",
      "Compute loss\n",
      "In (4, 1) (4, 1)  Out (4, 1)\n",
      "Start backward pass\n",
      "In  (4, 1) (1, 3)  Out (4, 3)\n",
      "In  (3, 4) (4, 1)  Out (3, 1)\n",
      "In  (2, 4) (4, 3)  Out (2, 3)\n",
      "Loss:  5.066439994622395\n",
      "***************\n",
      "Iteration No:  1\n",
      "Loss:  0.4959907791902342\n",
      "***************\n",
      "Iteration No:  2\n",
      "Loss:  0.4180671892167177\n",
      "***************\n",
      "Iteration No:  3\n",
      "Loss:  0.35298133007809646\n",
      "***************\n",
      "Iteration No:  4\n",
      "Loss:  0.2972549636567377\n",
      "***************\n",
      "Iteration No:  5\n",
      "Loss:  0.2492326038163328\n",
      "***************\n",
      "Iteration No:  6\n",
      "Loss:  0.20785392075862477\n",
      "***************\n",
      "Iteration No:  7\n",
      "Loss:  0.17231260916265176\n",
      "***************\n",
      "Iteration No:  8\n",
      "Loss:  0.14193744536652986\n",
      "***************\n",
      "Iteration No:  9\n",
      "Loss:  0.11613979792168384\n",
      "***************\n",
      "Final prediction: [[0.        ]\n",
      " [0.79896057]\n",
      " [0.23044545]\n",
      " [1.02940602]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Define data & target\n",
    "data = np.array([[0,0],[0,1],[1,0],[1,1]]) \n",
    "target = np.array([[0],[1],[0],[1]])\n",
    "\n",
    "print(\"Data shape\", data.shape)\n",
    "print(\"Target shape\", target.shape)\n",
    "\n",
    "# Definie initial weight matrix\n",
    "weights_0_1 = np.random.rand(2,3) \n",
    "weights_1_2 = np.random.rand(3,1)\n",
    "\n",
    "\n",
    "print(\"First weight layer shape\", weights_0_1.shape)\n",
    "print(\"Second weight layer shape\", weights_1_2.shape)\n",
    "print(\"\")\n",
    "\n",
    "# The print statement helps you check the dimensions of matrices and vectors\n",
    "for i in range(10):\n",
    "    print(\"Iteration No: \", i)\n",
    "          \n",
    "    # Step #1 - PREDICT (forward pass essentially)\n",
    "    if i == 0: print(\"Start forward pass\")\n",
    "    layer_1 = data.dot(weights_0_1)\n",
    "    if i == 0: print(\"In\", data.shape, weights_0_1.shape, \" Out\", layer_1.shape)\n",
    "    layer_2 = layer_1.dot(weights_1_2)\n",
    "    if i == 0: print(\"In\", layer_1.shape, weights_1_2.shape, \" Out\", layer_2.shape)\n",
    "    \n",
    "    \n",
    "    # Step #2 - COMPARE - COMPUTE THE LOSS\n",
    "    if i == 0: print(\"Compute loss\")\n",
    "    diff = (layer_2 - target) \n",
    "    if i == 0: print(\"In\", layer_2.shape, target.shape, \" Out\", diff.shape)\n",
    "    # Compute the squared loss, this is a scalar\n",
    "    loss = np.sum(diff*diff)         \n",
    "    \n",
    "    # Step #3 - BACK PROPAGATION\n",
    "    if i == 0: print(\"Start backward pass\")\n",
    "    layer_1_grad = diff.dot(weights_1_2.transpose()) \n",
    "    if i == 0: print(\"In \", diff.shape, weights_1_2.transpose().shape, \" Out\", layer_1_grad.shape)    \n",
    "    \n",
    "    weight_1_2_update = layer_1.transpose().dot(diff)\n",
    "    if i == 0: print(\"In \", layer_1.transpose().shape, diff.shape, \" Out\", weight_1_2_update.shape)    \n",
    "    \n",
    "    weight_0_1_update = data.transpose().dot(layer_1_grad)\n",
    "    if i == 0: print(\"In \", data.transpose().shape, layer_1_grad.shape, \" Out\", weight_0_1_update.shape)    \n",
    "    \n",
    "    learningRate = 0.1\n",
    "    weights_1_2 -= weight_1_2_update * learningRate\n",
    "    weights_0_1 -= weight_0_1_update * learningRate\n",
    "    print(\"Loss: \", loss)\n",
    "    print(\"***************\")\n",
    "\n",
    "layer_1 = data.dot(weights_0_1)    \n",
    "layer_2 = layer_1.dot(weights_1_2)    \n",
    "print(\"Final prediction:\", layer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Now let's use the framework we built.\n",
    "- We'll use exactly the same input.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (4, 2)\n",
      "Target shape (4, 1)\n",
      "[[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]]\n",
      "Iteration No:  0  loss:  [5.06643999]\n",
      "Iteration No:  1  loss:  [1.72520804]\n",
      "Iteration No:  2  loss:  [0.97072979]\n",
      "Iteration No:  3  loss:  [0.44845782]\n",
      "Iteration No:  4  loss:  [0.19705058]\n",
      "Iteration No:  5  loss:  [0.11889682]\n",
      "Iteration No:  6  loss:  [0.07853709]\n",
      "Iteration No:  7  loss:  [0.05072462]\n",
      "Iteration No:  8  loss:  [0.03190534]\n",
      "Iteration No:  9  loss:  [0.01958509]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Define data & target\n",
    "data = np.array([[0,0],[0,1],[1,0],[1,1]]) \n",
    "target = np.array([[0],[1],[0],[1]])\n",
    "\n",
    "print(\"Data shape\", data.shape)\n",
    "print(\"Target shape\", target.shape)\n",
    "\n",
    "# Definie initial weight matrix\n",
    "weights_0_1 = np.random.rand(2,3) \n",
    "weights_1_2 = np.random.rand(3,1)\n",
    "\n",
    "data = Tensor_V4(data, autograd=True) \n",
    "target = Tensor_V4(target, autograd=True)\n",
    "\n",
    "w = list()\n",
    "print(weights_0_1_orig)\n",
    "w.append(Tensor_V4(weights_0_1, autograd=True)) \n",
    "w.append(Tensor_V4(weights_1_2, autograd=True))\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Step #1 - PREDICT\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    \n",
    "    # Step #2 - COMPARE\n",
    "    loss = ((pred - target)*(pred - target)).sum(0) \n",
    "    print(\"Iteration No: \",i, \" loss: \", loss)\n",
    "    \n",
    "    # Step #3 - BACK PROPAGATION\n",
    "    loss.backward(Tensor_V4(np.ones_like(loss.data)))\n",
    "    \n",
    "    # Step #4 - UPDATE WEIGHTS\n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1 \n",
    "        w_.grad.data *= 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #6 - Stochastic gradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We'd like to further abstarct the weights update by writing a class. Nothing too complicated. Just rewriting with a bit of good, old-fashioned object-oriented programming.\n",
    "- In this way the update we wrote earlier can be further simplified.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha = 0.1): \n",
    "        self.parameters = parameters \n",
    "        self.alpha = alpha\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0 \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No:  0  loss:  [5.06643999]\n",
      "Iteration No:  1  loss:  [1.72520804]\n",
      "Iteration No:  2  loss:  [0.97072979]\n",
      "Iteration No:  3  loss:  [0.44845782]\n",
      "Iteration No:  4  loss:  [0.19705058]\n",
      "Iteration No:  5  loss:  [0.11889682]\n",
      "Iteration No:  6  loss:  [0.07853709]\n",
      "Iteration No:  7  loss:  [0.05072462]\n",
      "Iteration No:  8  loss:  [0.03190534]\n",
      "Iteration No:  9  loss:  [0.01958509]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "data = Tensor_V4(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True) \n",
    "target = Tensor_V4(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor_V4(weights_0_1, autograd=True)) \n",
    "w.append(Tensor_V4(weights_1_2, autograd=True))\n",
    "\n",
    "optim = SGD(parameters=w, alpha=0.1) \n",
    "for i in range(10):\n",
    "    \n",
    "            \n",
    "    # Step #1 - PREDICT\n",
    "    pred = data.mm(w[0]).mm(w[1])    \n",
    "    # Step #2 - COMPARE\n",
    "    loss = ((pred - target)*(pred - target)).sum(0) \n",
    "    # Step #3 - BACK PROPAGATION\n",
    "    loss.backward(Tensor_V4(np.ones_like(loss.data)))\n",
    "    \n",
    "    #---------NEW PART--------\n",
    "    # Step #4 - UPDATE WEIGHTS BUY ABSTACTED \n",
    "    optim.step()\n",
    "    print(\"Iteration No: \",i, \" loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #7 - Adding suppot for layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Layers are everywhere in Keras or PyTorch, they are implemented in a way that most of the heavy lifting is abstracted away. This makes them very easy to use. \n",
    "- **Layer abstraction** is a collection of commonly used forward propagation techniques packaged into an simple\n",
    "API with some kind of .forward() method to call them. \n",
    "- I created an **abstract class** `Layer`, which has a single getter. This allows for more-complicated layer \n",
    "types (such as layers containing other layers). All you need to do is override `get_parameters()` to \n",
    "control what tensors are later passed to the optimizer (such as the SGD class created in the previous \n",
    "section).\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self): \n",
    "        self.parameters = list()\n",
    "    def get_parameters(self): \n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        # Weights initialisation\n",
    "        W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs)) \n",
    "        self.weight = Tensor_V4(W, autograd = True)\n",
    "        self.bias = Tensor_V4(np.zeros(n_outputs), autograd = True)\n",
    "        self.parameters.append(self.weight) \n",
    "        self.parameters.append(self.bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight)+self.bias.expand(0, len(input.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #8 - Sequential architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The most popular architecture is a sequential layer \n",
    "- A sequential architecture forward propagates a list of layers, where each layer feeds its outputs into the inputs of the next layer. \n",
    "- Anther type of architecture is RNNs (Recurrent Neural Network)\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()): \n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer): \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input) \n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self): \n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Now that we have all this fancy OOP classes and methods, we are going back to our toy example.\n",
    "- This is to show how neatly we can write this. Generally neatly translates to: editable and short.\n",
    "- The results are NOT the same because inside the class `Linear()` we have intialised the weights in a different way.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data = Tensor_V4(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True) \n",
    "target = Tensor_V4(np.array([[0],[1],[0],[1]]), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.42951511]\n",
      "[15.70905316]\n",
      "[0.89190469]\n",
      "[0.16865993]\n",
      "[0.09771623]\n",
      "[0.05654691]\n",
      "[0.03260814]\n",
      "[0.01875125]\n",
      "[0.01076785]\n",
      "[0.00618446]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "optim = SGD(parameters = model.get_parameters(), alpha = 0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    loss.backward(Tensor_V4(np.ones_like(loss.data))) \n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 - Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- You can also create layers that are functions on the input. \n",
    "- The most popular version of this kind of layer is probably the loss-function layer, such as mean squared error:\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        super() allows you to get access to the methods\n",
    "        if the inherited class Layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target)*(pred - target)).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data = Tensor_V4(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True) \n",
    "target = Tensor_V4(np.array([[0],[1],[0],[1]]), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.38617359]\n",
      "[1.55744639]\n",
      "[0.81051793]\n",
      "[0.43579924]\n",
      "[0.2427959]\n",
      "[0.13654357]\n",
      "[0.07732922]\n",
      "[0.04422531]\n",
      "[0.02567524]\n",
      "[0.01520718]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "criterion = MSELoss()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    # Compare\n",
    "    loss = criterion.forward(pred, target)\n",
    "    # Learn\n",
    "    loss.backward(Tensor_V4(np.ones_like(loss.data))) \n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 - Nonlinearity layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-  Let's add some non-linear ativation functions:\n",
    "    - `sigmoid()` \n",
    "    - `tanh()`\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor_V5 (object):\n",
    "    \n",
    "    \"\"\"Tensor class\n",
    "    \n",
    "    Stores all the numerical information in a NumPy array (self.data), and it \n",
    "    supports one tensor operation (addition).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, \n",
    "                 autograd=False,\n",
    "                 creators=None, \n",
    "                 creation_op=None,\n",
    "                id=None):\n",
    "        \n",
    "        \"\"\"__init__ method\n",
    "        \n",
    "        Given a list makes it into an array\n",
    "        Each tensor has now two new attributes:\n",
    "        creators   :: list containing any tensors used in the creation of the\n",
    "                      current tensor (which defaults to None). \n",
    "        creaion_op :: \n",
    "        \"\"\"\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd \n",
    "        self.children = {}\n",
    "        if(id is None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        \n",
    "        # Keeping track of how many children a tensor has\n",
    "        if(creators is not None): \n",
    "            for c in creators:\n",
    "                if(self.id not in c.children): \n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        \"\"\"\n",
    "        Checks whether a tensor has received the \n",
    "        correct number of gradients from each child\n",
    "        \"\"\"\n",
    "\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True        \n",
    "\n",
    "    \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        \n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None): \n",
    "                # Checks to make sure you can backpropagate\n",
    "                # or wheter you are waiting for a gradient\n",
    "                # in which case decrement the counter\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None): \n",
    "                # Accumulates gradients from several children\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "                \n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):    \n",
    "                if(self.creation_op == \"add\"):        \n",
    "                    self.creators[0].backward(self.grad, self) \n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "\n",
    "                if(self.creation_op == \"neg\"): \n",
    "                    self.creators[0].backward(self.grad.__neg__()) \n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new = Tensor_V5(self.grad.data) \n",
    "                    self.creators[0].backward(new, self) \n",
    "                    new = Tensor_V5(self.grad.__neg__().data) \n",
    "                    self.creators[1].backward(new, self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1] \n",
    "                    self.creators[0].backward(new , self) \n",
    "                    new = self.grad * self.creators[0] \n",
    "                    self.creators[1].backward(new, self)\n",
    "\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    act = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose()) \n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose() \n",
    "                    weights.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"): \n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                \n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim] \n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                \n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1]) \n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor_V5(np.ones_like(self.grad.data)) \n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor_V5(np.ones_like(self.grad.data)) \n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \n",
    "        \"\"\"__add__ method\n",
    "        \n",
    "        “+” is just syntactic sugar around the “__add__” method\n",
    "        so x+y is equivalent to x.__add__(y).\n",
    "        \n",
    "        ensors x and y are added together, z has two creators\n",
    "        \"\"\"\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor_V5(self.data + other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other], \n",
    "                          creation_op=\"add\" )\n",
    "        return Tensor_V5(self.data + other.data)\n",
    "    \n",
    "    def __neg__(self): \n",
    "        if(self.autograd):\n",
    "            return Tensor_V5(self.data * -1, \n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor_V5(self.data * -1)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor_V5(self.data - other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\") \n",
    "        return Tensor_V5(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other): \n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor_V5(self.data * other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\") \n",
    "        return Tensor_V5(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dim): \n",
    "        if(self.autograd):\n",
    "            return Tensor_V5(self.data.sum(dim), \n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim)) \n",
    "        return Tensor_V5(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "        trans_cmd = list(range(0,len(self.data.shape))) \n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape) \n",
    "        \n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        if(self.autograd):\n",
    "            return Tensor_V5(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op = \"expand_\" + str(dim))\n",
    "        return Tensor_V5(new_data)\n",
    "    \n",
    "    def transpose(self): \n",
    "        if(self.autograd):\n",
    "            return Tensor_V5(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self], \n",
    "                          creation_op = \"transpose\")\n",
    "        return Tensor_V5(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x): \n",
    "        if(self.autograd):\n",
    "            return Tensor_V5(self.data.dot(x.data), \n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\") \n",
    "        return Tensor_V5(self.data.dot(x.data))\n",
    "\n",
    "    def __repr__(self):                \n",
    "        \"\"\"__repr__ method\n",
    "        Returns the object representation in string format.\n",
    "        Returns an “official” string representation of the object,\n",
    "        which can be used to construct th eobject again.\n",
    "        \"\"\"\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns the string representation of the object. This method is called \n",
    "        when print() or str() function is invoked on an object. Returns a\n",
    "        human-readable format.\n",
    "        \"\"\"\n",
    "        return str(self.data.__str__())\n",
    "\n",
    "    def tanh(self): \n",
    "        if(self.autograd):\n",
    "            return Tensor_V5(np.tanh(self.data), \n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\") \n",
    "        return Tensor_V5(np.tanh(self.data))\n",
    "\n",
    "    def sigmoid(self): \n",
    "        if(self.autograd):\n",
    "            return Tensor_V5(1 / (1 + np.exp(-self.data)), \n",
    "                               autograd=True,\n",
    "                                creators=[self], \n",
    "                                creation_op=\"sigmoid\")\n",
    "        return Tensor_V51 / (1 + np.exp(-self.data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        # test\n",
    "        return input.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input): \n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Let's now revisit the code we wrote before.\n",
    "- The framework created above allows you to drop the `Tanh()` and `Sigmoid()` layers into the input parameters to Sequential(), and the neural network knows exactly how to use them.\n",
    "- That allows you to write in a neaty way each layer.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data = Tensor_V5(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True) \n",
    "target = Tensor_V5(np.array([[0],[1],[0],[1]]), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2,3), Tanh(), Linear(3,1), Sigmoid()]) \n",
    "criterion = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06372865]\n",
      "[0.75148144]\n",
      "[0.57384259]\n",
      "[0.39574294]\n",
      "[0.2482279]\n",
      "[0.15515294]\n",
      "[0.10423398]\n",
      "[0.07571169]\n",
      "[0.05837623]\n",
      "[0.04700013]\n"
     ]
    }
   ],
   "source": [
    "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "for i in range(10):\n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    # Compare\n",
    "    loss = criterion.forward(pred, target)\n",
    "    # Learn\n",
    "    loss.backward(Tensor_V5(np.ones_like(loss.data))) \n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Grokking deep learning, Andrew W. Task \n",
    "- [Code available in this GitHub repository](https://github.com/iamtrask/Grokking-Deep-Learning)\n",
    "- [Available on Google books](https://books.google.co.uk/books?hl=en&lr=&id=0zczEAAAQBAJ&oi=fnd&pg=PT25&dq=Grokking-Deep-Learning&ots=-kAj0RlLxI&sig=P2qD8ZSvOYc6k9yttDfebqbgnec#v=onepage&q=Grokking-Deep-Learning&f=false)\n",
    "- [Difference btw str and repr](https://www.journaldev.com/22460/python-str-repr-functions)\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We have seen how building a framework is all about abstracting to the bare minimum. \n",
    "- One of the thing we need to be careful about was the updating process as it easy to debug it if something is not quite right.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
