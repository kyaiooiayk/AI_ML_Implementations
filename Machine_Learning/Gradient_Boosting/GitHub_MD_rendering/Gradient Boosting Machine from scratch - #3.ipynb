{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Building a gradient boost algorithm from scratch\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
""      ],
      "text/plain": [
       "   instant  season   yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0        1     1.0  0.0     1      0.0      6.0         0.0           2   \n",
       "1        2     1.0  0.0     1      0.0      0.0         0.0           2   \n",
       "2        3     1.0  0.0     1      0.0      1.0         1.0           1   \n",
       "\n",
       "       temp     atemp       hum  windspeed   cnt  \n",
       "0  0.344167  0.363625  0.805833   0.160446   985  \n",
       "1  0.363478  0.353739  0.696087   0.248539   801  \n",
       "2  0.196364  0.189405  0.437273   0.248309  1349  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bikes = pd.read_csv('../DATASETS/bike_rentals_cleaned.csv')\n",
    "df_bikes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X and y\n",
    "X_bikes = df_bikes.iloc[:,:-1]\n",
    "y_bikes = df_bikes.iloc[:,-1]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a gradient boosting model from scratch in 7 steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The initial decision tree, called a base learner, should not be fine-tuned for accuracy. \n",
    "- We want a model thatfocuses on learning from errors, not a model that relies heavily on the base learner.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 - Initialize Decision Tree Regressor and fit to training data\n",
    "tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Make predictions with the training set: Instead of making predictions with the test set, predictions in gradient boosting are initially made with the training set. \n",
    "- Why? To compute the residuals, we need to compare the predictions while still in the training phase. \n",
    "- The test phase of the model build comes at the end, after all thetrees have been constructed.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Make predictions on training set\n",
    "y_train_pred = tree_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Compute the residuals: The residuals are the differences between the predictions and the target column.\n",
    "- The residuals are defined as y2_train because they are the new target column for the next tree. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Compute residuals\n",
    "y2_train = y_train - y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Fit the new tree on the residuals: Fitting a new tree on the residuals is different than fitting a model on the training set. The primary difference is in the predictions. \n",
    "- In the bike rentals dataset, when fitting a new tree on the residuals, we should progressively get smaller numbers.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4 - Initialize Decision Tree Regressor and fit tree to training data\n",
    "tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "tree_2.fit(X_train, y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Repeat steps 2-4: As the process continues, the residuals should gradually approach 0 from the positive and negative direction. \n",
    "- This process may continue for dozens, hundreds, or thousands of trees. \n",
    "- Under normal circumstances, you would certainly keep going. \n",
    "- It will take more than a few trees to transform a weak learner into a strong learner. \n",
    "- Since our goal is to understand how gradient boosting works behind the scenes, however, we will move on now that the general idea has been covered.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5 - Repeat steps from 2 to 4\n",
    "\n",
    "# Make predictions on training set\n",
    "y2_train_pred = tree_2.predict(X_train)\n",
    "# Compute residuals\n",
    "y3_train = y2_train - y2_train_pred\n",
    "# Initialize Decision Tree Regressor\n",
    "tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)\n",
    "# Fit tree to training data\n",
    "tree_3.fit(X_train, y3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Sum the results: Summing the results requires making predictions for each tree with the test set. \n",
    "- Since the predictions are positive and negative differences, summing the predictions should result in predictions that are closer to the target.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - Sum the results\n",
    "y1_pred = tree_1.predict(X_test)\n",
    "y2_pred = tree_2.predict(X_test)\n",
    "y3_pred = tree_3.predict(X_test)\n",
    "y_pred = y1_pred + y2_pred + y3_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911.0479538776444"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7 - Compute root mean squared error (rmse)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a gradient boosting via scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Let's see whether we can obtain the same result as in the previous section using scikit-learn's GradientBoostingRegressor. \n",
    "- To obtain the same results, it's essential to match max_depth=2 and random_state=2. \n",
    "- Furthermore, since there are only three trees, we must have n_estimators=3. \n",
    "- Finally, we must set the learning_rate=1.0 hyperparameter. \n",
    "- As you can the result pratically the same!\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911.0479538776439"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the mthod\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=2, learning_rate=1.0)\n",
    "# Fit on the training data\n",
    "gbr.fit(X_train, y_train)\n",
    "# Predict test data\n",
    "y_pred = gbr.predict(X_test)\n",
    "# Compute root mean squared error (rmse)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Recall that the point of gradient boosting is to build a model with enough trees to transform a weak learner into a strong learner. \n",
    "- This is easily done by changing n_estimators, the number of iterations, to a much larger number.\n",
    "- Let's build and score a gradient boosting regressor with 30 estimators:\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857.1072323426944"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=30, random_state=2, learning_rate=1.0)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936.3617413678853"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Now, we changed learning_rate without saying much about it. \n",
    "- So, what happens if we remove learning_rate=1.0 and use the scikit-learn defaults?\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653.7456840231495"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying gradient boosting hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- learning_rate, also known as the shrinkage, shrinks the contribution of individual trees so that no tree  has too much influence when building the model. \n",
    "- If an entire ensemble is built from the errors of one base learner, without careful adjustment of hyperparameters, early trees in the model can have too much influence on subsequent development.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001 , Score: 1633.0261400367258\n",
      "Learning Rate: 0.01 , Score: 831.5430182728547\n",
      "Learning Rate: 0.05 , Score: 685.0192988749717\n",
      "Learning Rate: 0.1 , Score: 653.7456840231495\n",
      "Learning Rate: 0.15 , Score: 687.666134269379\n",
      "Learning Rate: 0.2 , Score: 664.312804425697\n",
      "Learning Rate: 0.3 , Score: 689.4190385930236\n",
      "Learning Rate: 0.5 , Score: 693.8856905068778\n",
      "Learning Rate: 1.0 , Score: 936.3617413678853\n"
     ]
    }
   ],
   "source": [
    "learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]\n",
    "for value in learning_rate_values:\n",
    "    gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=value)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Learning Rate:', value, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: None , Score: 869.2783041945797\n",
      "Max Depth: 1 , Score: 707.8261886858736\n",
      "Max Depth: 2 , Score: 653.7456840231495\n",
      "Max Depth: 3 , Score: 646.4045923317708\n",
      "Max Depth: 4 , Score: 663.048387855927\n"
     ]
    }
   ],
   "source": [
    "depths = [None, 1, 2, 3, 4]\n",
    "for depth in depths:\n",
    "    gbr = GradientBoostingRegressor(max_depth=depth, n_estimators=300, random_state=2)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Max Depth:', depth, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Subsample is a subset of samples. \n",
    "- Since samples are the rows, a subset of rows means that all rows may not be included when building each tree. \n",
    "- By changing subsample from 1.0 to a smaller decimal, trees only select that percentage of samples during the build phase. \n",
    "- For example, subsample=0.8 would select 80% of samples for each tree.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsample: 1 , Score: 646.4045923317708\n",
      "Subsample: 0.9 , Score: 620.1819001443569\n",
      "Subsample: 0.8 , Score: 617.2355650565677\n",
      "Subsample: 0.7 , Score: 612.9879156983139\n",
      "Subsample: 0.6 , Score: 622.6385116402317\n",
      "Subsample: 0.5 , Score: 626.9974073227554\n"
     ]
    }
   ],
   "source": [
    "samples = [1, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "for sample in samples:\n",
    "    gbr = GradientBoostingRegressor(max_depth=3, n_estimators=300, subsample=sample, random_state=2)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    rmse = MSE(y_test, y_pred)**0.5\n",
    "    print('Subsample:', sample, ', Score:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We can use what we learned from above to fix some starting values for a more thourgh search. \n",
    "- With 27 possible combinations of hyperparameters, we use RandomizedSearchCV to try 10 of these combinations in the hopes of finding a good model. \n",
    "- While 27 combinations are feasible with GridSearchCV, at some point you will end up with too many possibilities and RandomizedSearchCV will become essential.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.65, 'n_estimators': 300, 'learning_rate': 0.05}\n",
      "Training score: 636.200\n",
      "Test set score: 625.985\n"
     ]
    }
   ],
   "source": [
    "params={'subsample':[0.65, 0.7, 0.75],\n",
    "                          'n_estimators':[300, 500, 1000],\n",
    "                          'learning_rate':[0.05, 0.075, 0.1]\n",
    "                         }\n",
    "\n",
    "# Import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbr = GradientBoostingRegressor(max_depth=3, random_state=2)\n",
    "\n",
    "\n",
    "# Instantiate RandomizedSearchCV as rand_reg\n",
    "rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error', \n",
    "                              cv=5, n_jobs=-1, random_state=2)\n",
    "\n",
    "# Fit grid_reg on X_train and y_train\n",
    "rand_reg.fit(X_train, y_train)\n",
    "# Extract best estimator\n",
    "best_model = rand_reg.best_estimator_\n",
    "# Extract best params\n",
    "best_params = rand_reg.best_params_\n",
    "# Print best params\n",
    "print(\"Best params:\", best_params)\n",
    "# Compute best score\n",
    "best_score = np.sqrt(-rand_reg.best_score_)\n",
    "\n",
    "# Print best score\n",
    "print(\"Training score: {:.3f}\".format(best_score))\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test, y_pred)**0.5\n",
    "# Print rmse_test\n",
    "print('Test set score: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error:  596.9544588974487\n",
      "Train error:  159.90255545058218\n"
     ]
    }
   ],
   "source": [
    "# After a few rounds of experimentation, we obtained the following model.\n",
    "gbr = GradientBoostingRegressor(max_depth=3, n_estimators=1600, subsample=0.75, learning_rate=0.02, random_state=2)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "print(\"Test error: \", MSE(y_test, y_pred)**0.5)\n",
    "\n",
    "y_pred1 = gbr.predict(X_train)\n",
    "print(\"Train error: \", MSE(y_train, y_pred1)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- XGBoost is preferred over gradient boosting in general because it consistently delivers better results. \n",
    "- It is essentially a better vesion. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584.339544309016"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the XGBRegressor, xg_reg\n",
    "xg_reg = XGBRegressor(max_depth=3, n_estimators=1600, eta=0.02, subsample=0.75, random_state=2)\n",
    "\n",
    "# Fit xg_reg to training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels of test set, y_pred\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute root mean squared error (rmse)\n",
    "MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Corey Wade. â€œHands-On Gradient Boosting with XGBoost and scikit-learn\n",
    "- https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
