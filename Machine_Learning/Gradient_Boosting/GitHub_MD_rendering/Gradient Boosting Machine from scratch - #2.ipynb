{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Gradient Boosting Machine from scratch. Implementation #2\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- pip install the following modules:\n",
    "    - pip install drawtree\n",
    "    - pip install --upgrade \"jax[cpu]\"\n",
    "    - pip install jax\n",
    "    - pip install asciitree\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asciitree import LeftAligned\n",
    "from collections import OrderedDict as OD\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from jax import grad, jacfwd, jacrev, jit\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from drawtree import draw_level_order\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some info on the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- To support any kind of objective, without the pain of manually calculating the gradient and the hessian, we use an AD ibrary called **jax**.\n",
    "- Purely arbitrarly, **features are selected randomly** but a better option is to use features having the greatest variance.\n",
    "<br><br>\n",
    "- Step-by-step instruction to train the decision trees:\n",
    "    - Pick a feature of interest\n",
    "    - Order data points attached to the current node using values of the selected feature\n",
    "    - Pick a possible split value\n",
    "    - Put data points below this split value in the right node, and the one above in the left node\n",
    "    - Compute objective reduction for the parent node, the right node and the left one\n",
    "    - If the sum of the objective reduction for left and right nodes is greater than the one of the parent node, keep the split value as the best one\n",
    "    - Iterate for each split value\n",
    "    - Use the best split value if any, and add the two new nodes\n",
    "    - If no splitting improved the objective, donâ€™t add child nodes.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(fun):\n",
    "    return jit(jacfwd(jacrev(fun)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    \"\"\"\n",
    "    Node decision class.\n",
    "    This is a simple binary node, with potentially two childs: left and right\n",
    "    Left node is returned when condition is true\n",
    "    False node is returned when condition is false<\n",
    "    \"\"\"\n",
    "    def __init__(self, name, condition, value=None):\n",
    "        self.name = name\n",
    "        self.condition = condition\n",
    "        self.value = value\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def add_left_node(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def add_right_node(self, right):\n",
    "        self.right = right\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Node is a leaf if it has no child\n",
    "        \"\"\"\n",
    "        return (not self.left) and (not self.right)\n",
    "\n",
    "    def next(self, data):\n",
    "        \"\"\"\n",
    "        Return next code depending on data and node condition\n",
    "        \"\"\"\n",
    "        cond = self.condition(data)\n",
    "        if cond:\n",
    "            return self.left\n",
    "        else:\n",
    "            return self.right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    A DecisionTree is a model that provides predictions depending on input.\n",
    "    Prediction is the sum of the values attached to leaf activated by input\n",
    "    \"\"\"\n",
    "    def __init__(self, objective, nb_estimators, max_depth):\n",
    "        \"\"\"\n",
    "        A DecisionTree is defined by an objective, a number of estimators and a max depth.\n",
    "        \"\"\"\n",
    "        self.roots = [DecisionNode(f'root_{esti}', None, 0.0) for esti in range(0, nb_estimators)]\n",
    "        self.objective = objective\n",
    "        self.lbda = 0.0\n",
    "        self.gamma = 1.0 * 0\n",
    "        self.grad = grad(self.objective)\n",
    "        self.hessian = hessian(self.objective)\n",
    "        self.max_depth = max_depth\n",
    "        self.base_score = None\n",
    "\n",
    "\n",
    "    def _create_condition(self, col_name, split_value):\n",
    "        \"\"\"\n",
    "        Create a closure that capture split value\n",
    "        \"\"\"\n",
    "        return lambda dta : dta[col_name] < split_value\n",
    "\n",
    "    def _pick_columns(self, columns):\n",
    "        return random.choice(columns)\n",
    "\n",
    "    def _add_child_nodes(self, node, nodes,\n",
    "                         node_x, node_y,\n",
    "                         split_value, split_column,\n",
    "                         nb_nodes,\n",
    "                         left_w, right_w, prev_w):\n",
    "        node.name = f'{split_column} < {split_value}'\n",
    "        node.condition = self._create_condition(split_column, split_value) # we must create a closure to capture split_value copy\n",
    "        node.add_left_node(DecisionNode(f'left_{nb_nodes} - {split_column} < {split_value}',\n",
    "                                        None, left_w + prev_w))\n",
    "        node.add_right_node(DecisionNode(f'right_{nb_nodes} - {split_column} >= {split_value}',\n",
    "                                         None, right_w + prev_w))\n",
    "        mask = node_x[split_column] < split_value\n",
    "        # Reverse order to ensure bfs\n",
    "        nodes.append((node.left,\n",
    "                      node_x[mask].copy(),\n",
    "                      node_y[mask].copy(),\n",
    "                      left_w + prev_w))\n",
    "        nodes.append((node.right,\n",
    "                      node_x[~mask].copy(),\n",
    "                      node_y[~mask].copy(),\n",
    "                      right_w + prev_w))\n",
    "\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit decision trees using x_train and objective\n",
    "        \"\"\"\n",
    "        self.base_score = y_train.mean()\n",
    "        for tree_idx, tree_root in enumerate(self.roots):\n",
    "            # store current node (currenly a lead), x_train and node leaf weight\n",
    "            nodes = [(tree_root, x_train.copy(), y_train.copy(), 0.0)]\n",
    "            nb_nodes = 0\n",
    "            # Add node to tree using bfs\n",
    "            while nodes:\n",
    "                node, node_x, node_y, prev_w = nodes.pop(0)\n",
    "                node_x['pred'] = self.predict(node_x)\n",
    "                split_column = self._pick_columns(x_train.columns) # XGBoost use a smarter heuristic here\n",
    "                best_split, split_value, left_w, right_w = self._find_best_split(split_column,\n",
    "                                                                                 node_x, node_y)\n",
    "                if best_split != -1:\n",
    "                    self._add_child_nodes(node, nodes,\n",
    "                                          node_x, node_y,\n",
    "                                          split_value, split_column,\n",
    "                                          nb_nodes,\n",
    "                                          left_w, right_w, prev_w)\n",
    "                nb_nodes += 1\n",
    "                if nb_nodes >= 2**self.max_depth-1:\n",
    "                    break\n",
    "\n",
    "\n",
    "    def _gain_and_weight(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Compute gain and leaf weight using automatic differentiation\n",
    "        \"\"\"\n",
    "        pred = x_train['pred'].values\n",
    "        G_i = self.grad(pred, y_train.values).sum()\n",
    "        H_i = self.hessian(pred, y_train.values).sum()\n",
    "        return -0.5 * G_i * G_i / (H_i + self.lbda) + self.gamma, -G_i / (H_i + self.lbda)\n",
    "\n",
    "    def _find_best_split(self, col_name, node_x, node_y):\n",
    "        \n",
    "        \"\"\" Compute best split.\n",
    "        \n",
    "        This is the core of the method.\n",
    "        \"\"\"\n",
    "        x_sorted = node_x.sort_values(by=col_name)\n",
    "        y_sorted = node_y[x_sorted.index]\n",
    "        current_gain, _ = self._gain_and_weight(x_sorted, node_y)\n",
    "        gain = 0.0\n",
    "        best_split = -1\n",
    "        split_value, best_left_w, best_right_w = None, None, None\n",
    "        for split_idx in range(1, x_sorted.shape[0]):\n",
    "            left_data = x_sorted.iloc[:split_idx]\n",
    "            right_data = x_sorted.iloc[split_idx:]\n",
    "            left_y = y_sorted.iloc[:split_idx]\n",
    "            right_y = y_sorted.iloc[split_idx:]\n",
    "            left_gain, left_w = self._gain_and_weight(left_data, left_y)\n",
    "            right_gain, right_w = self._gain_and_weight(right_data, right_y)\n",
    "            if current_gain - (left_gain + right_gain) > gain:\n",
    "                gain = current_gain - (left_gain + right_gain)\n",
    "                best_split = split_idx\n",
    "                split_value = x_sorted[col_name].iloc[split_idx]\n",
    "                best_left_w = left_w\n",
    "                best_right_w = right_w\n",
    "        return best_split, split_value, best_left_w, best_right_w\n",
    "\n",
    "    def predict(self, data):\n",
    "        preds = []\n",
    "        for _, row in data.iterrows():\n",
    "            pred = 0.0\n",
    "            for tree_idx, root in enumerate(self.roots):\n",
    "                child = root\n",
    "                while child and not child.is_leaf():\n",
    "                    child = child.next(row)\n",
    "                pred += child.value\n",
    "            preds.append(pred)\n",
    "        return np.array(preds) + self.base_score\n",
    "\n",
    "    def show(self):\n",
    "        print('not yet implemented')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y_pred, y_true):\n",
    "    diff = y_true - y_pred\n",
    "    return jnp.dot(diff, diff.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6. 7.]\n",
      "[1. 2. 3. 4. 5. 6. 7.]\n",
      "[1.        2.        3.        4.        5.        5.9999995 7.       ]\n",
      "[1.  2.5 3.  4.5 5.  6.5 7. ]\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.DataFrame({\"A\" : [3.0, 2.0, 1.0, 4.0, 5.0, 6.0, 7.0]})\n",
    "y_train = pd.DataFrame({\"Y\" : [3.0, 2.0, 1.0, 4.0, 5.0, 6.0, 7.0]})\n",
    "\n",
    "tree = DecisionTree(squared_error, 1, 3)\n",
    "tree.fit(x_train, y_train['Y'])\n",
    "pred = tree.predict(pd.DataFrame({'A': [1., 2., 3., 4., 5., 6., 7.]}))\n",
    "print(pred) #-> [1. 2. 3. 4. 5. 6. 7.]\n",
    "\n",
    "tree = DecisionTree(squared_error, 2, 3)\n",
    "tree.fit(x_train, y_train['Y'])\n",
    "pred = tree.predict(pd.DataFrame({'A': [1., 2., 3., 4., 5., 6., 7.]}))\n",
    "print(pred) #-> [1. 2. 3. 4. 5. 6. 7.]\n",
    "\n",
    "tree = DecisionTree(squared_error, 4, 2)\n",
    "tree.fit(x_train, y_train['Y'])\n",
    "pred = tree.predict(pd.DataFrame({'A': [1., 2., 3., 4., 5., 6., 7.]}))\n",
    "print(pred) # -> [1.        2.        3.        4.        5.        5.9999995 7.       ]\n",
    "\n",
    "x_train = pd.DataFrame({'A': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0,\n",
    "                              1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0,],\n",
    "                        'B': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "                              1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,]})\n",
    "y_train = pd.DataFrame({\"Y\" : [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0,\n",
    "                               1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5]})\n",
    "\n",
    "tree = DecisionTree(squared_error, 1, 6)\n",
    "tree.fit(x_train, y_train['Y'])\n",
    "pred = tree.predict(pd.DataFrame({'A': [1., 2., 3., 4., 5., 6., 7.],\n",
    "                                  'B': [0., 1., 0., 1., 0., 1., 0.]}))\n",
    "print(pred) #-> [1.  2.5 3.  4.5 5.  6.5 7. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- https://towardsdatascience.com/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
